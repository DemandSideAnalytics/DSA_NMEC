{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Instructions and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example walks through how to produce a Time-of-Week Temperature regression on individual premise hourly or daily electric consumption data. There are two regression methods:\n",
    "* Daily - use a day of week dummy (0-6) in the regression\n",
    "* Hourly - use an hour of week dummy (1-168) in the regression\n",
    "\n",
    "You may also choose one of two spline construction methods:\n",
    "* Static - the cut points of the temperature bins are fixed at 50, 60, and 70F for four bins\n",
    "* Dynamic - the cut points are pruned as discussed in the CalTRACK implementation (Section 3.9)\n",
    "\n",
    "The parameters definining variable names and regression parameters are defined in the splineargs dictionary:\n",
    "* idvars (string): the ID variable. Typically something like prem_uuid\n",
    "* date (string): the date variable. It's converted to a datetime in the data prep function\n",
    "* hour (int): hour ending 1-24\n",
    "* treatvar (int): bool 0/1 indicating treatment. We only keep the treat == 0 in this application.\n",
    "* gps (list of strings): any column names in your DF that represent granular profiles. For this application, this should remain empty in your CVRMSE assessment\n",
    "* usagevar (string): name of usage variable in your DF (kWh here)\n",
    "* tempvar (string): name of the temperature variable in your DF. Temperature must be in Fahrenheit\n",
    "* splinemethod (string): either 'static' or 'dynamic'\n",
    "* dailybool (boolean): True if the data should be converted to daily averages (note that the data coming in should remain hourly, and the data is converted in data prep function). \n",
    "\n",
    "The general procedure is to:\n",
    "1. Load the data\n",
    "2. Prep the data (remove NAN/NULL, convert optionally to daily)\n",
    "3. Construct the spline - either static or dynamic\n",
    "4. Produce Hour of Week/Day of Week dummies\n",
    "5. Run the regression to fit the model and predict in-sample (on the same observations that were used to fit the model)\n",
    "6. Compute the CVRMSE of the predictions\n",
    "\n",
    "Steps 1, 2, and 6 can be done on a dataset with multiple customers/premises in it. Steps 3, 4, and 5 must be done on a dataframe that contains only one customer at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the directories to the relevant files and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be paths to the accompanying data file we send. Once you run yourself, you can modify as long as the structure of the \n",
    "# data file is similar to this\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "basepath = \"D:/Projects/PG&E/2023-2025 OBF P4P NMEC Payable Savings/\"\n",
    "dir2 = \"Deliverables/05_Open_Source_Functions/01_Example_Data/\"\n",
    "analysis =\"Example_Hourly_Data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Prepped Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df, splineargs):\n",
    "    '''\n",
    "    Takes the raw data and applies cleaning. Removes NAN/Nulls, collapses\n",
    "    to a daily value if required by regression, and constructs the seasons\n",
    "    needed for the seasonal regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        data frame (no index) with the columns\n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        dataframe that has been cleaned with an additional variable '_season'\n",
    "        based on the splinearg['seasons'] mapping of months to seasons\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Parse the spline parameters\n",
    "    idvar = splineargs['idvars']\n",
    "    date = splineargs['date']\n",
    "    hour = splineargs['hour']\n",
    "    dailybool = splineargs['dailybool']\n",
    "    seasons = splineargs['seasons']\n",
    "    \n",
    "    # Clean the data\n",
    "    df[date] = pd.to_datetime(df[date], format=\"%m/%d/%Y\")\n",
    "\n",
    "    # Assign seasons to the data\n",
    "    df['_season'] = df[date].dt.month.apply(str).map(seasons)\n",
    "\n",
    "    # Remove any missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Check if daily and group mean by ID & date if so\n",
    "    if dailybool:\n",
    "        df = df.groupby([idvar, date]).mean()\n",
    "        df = df.drop([hour], axis = 1)\n",
    "    \n",
    "    # Convert back to numeric index\n",
    "    df = df.reset_index()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Spline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_spline(df, splineargs):\n",
    "    '''\n",
    "    Constructs a temperature spline with fixed cutpoints\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Dataframe that has the required temperature var for spline\n",
    "        construction. Note that 'tempvar' must be a column in 'df'\n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : Pandas dataframe with the spline variables added\n",
    "    binlist : list of the column names in df that contain the spline vars\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Validate that  tempvar exists in column\n",
    "    tempvar = splineargs['tempvar']\n",
    "    if tempvar not in df.columns:\n",
    "        raise KeyError('{} not in dataframe'.format(tempvar))\n",
    "        \n",
    "    df['bin_0'] = np.minimum(50.0, df[tempvar])  \n",
    "    df['bin_1'] = np.where(df[tempvar] >= 50.0, np.minimum(10.0, df[tempvar]-50.0), 0.0)\n",
    "    df['bin_2'] = np.where(df[tempvar] >= 60.0, np.minimum(10.0, df[tempvar]-60.0), 0.0)\n",
    "    df['bin_3'] = np.maximum(0.0, df[tempvar]-70.0)  \n",
    "    \n",
    "    binlist = ['bin_{}'.format(i) for i in range(4)]\n",
    "    \n",
    "    return df, binlist\n",
    "\n",
    "\n",
    "\n",
    "def dsa_temperature_bin(df, splineargs):\n",
    "    '''\n",
    "    Using input data, constructs spline cutpoint values for each site\n",
    "    dynamically, ensuring that each bin has sufficient temperature\n",
    "    values for analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Dataframe that has the required temperature var for spline\n",
    "        construction. Note that 'tempvar' must be a column in 'df'. The \n",
    "        df may have one or more sites/idvar levels represented. \n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : Pandas dataframe with the spline variables added\n",
    "    binlist : list of the column names in df that contain the spline vars\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Set up \n",
    "    \n",
    "    # Parse the spline arguments reuired for the analysis\n",
    "    temperature_var = splineargs['tempvar']\n",
    "    id_var = splineargs['idvars']\n",
    "    mincount = splineargs['mintempcount']\n",
    "    \n",
    "\n",
    "    # D. Generate the starting temperature bins\n",
    "    bins = [float('-inf'), 30, 45, 55, 65, 75, 90, float('inf')]\n",
    "    bin_labels = range(1, len(bins)) # range(1, len(bins) + 1) \n",
    "    \n",
    "    df['_bin'] = pd.cut(df[temperature_var], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "    \n",
    "    # Get the counts of temperature values in each original bin range\n",
    "    df['_count_'] = 1\n",
    "    df = df[[id_var, '_bin', '_count_']].groupby([id_var, '_bin']).sum().reset_index()\n",
    "\n",
    "    # Make sure we have full bin coverage (fill in bins 0-7 for each site)\n",
    "    fullbins = pd.DataFrame({'_bin': range(1, 8)})\n",
    "    fullbins = df[[id_var]].drop_duplicates().merge(fullbins, how='cross')\n",
    "    df = pd.merge(fullbins, df, how='outer', on=[id_var, '_bin'])\n",
    "    \n",
    "    # Add in the bin cutpoint specifications\n",
    "    df['_lb'] = np.where((df['_bin'] == 1), float('-inf'), np.NAN)\n",
    "    df['_lb'] = np.where((df['_bin'] == 2), 30.00001, df['_lb'])\n",
    "    df['_lb'] = np.where((df['_bin'] == 3), 45.00001, df['_lb'])\n",
    "    df['_lb'] = np.where((df['_bin'] == 4), 55.00001, df['_lb'])\n",
    "    df['_lb'] = np.where((df['_bin'] == 5), 65.00001, df['_lb'])\n",
    "    df['_lb'] = np.where((df['_bin'] == 6), 75.00001, df['_lb'])\n",
    "    df['_lb'] = np.where((df['_bin'] == 7), 90.00001, df['_lb'])\n",
    "    \n",
    "    df['_ub'] = np.where((df['_bin'] == 1), 30, np.NAN)\n",
    "    df['_ub'] = np.where((df['_bin'] == 2), 45, df['_ub'])\n",
    "    df['_ub'] = np.where((df['_bin'] == 3), 55, df['_ub'])\n",
    "    df['_ub'] = np.where((df['_bin'] == 4), 65, df['_ub'])\n",
    "    df['_ub'] = np.where((df['_bin'] == 5), 75, df['_ub'])\n",
    "    df['_ub'] = np.where((df['_bin'] == 6), 90, df['_ub'])\n",
    "    df['_ub'] = np.where((df['_bin'] == 7), float('inf'), df['_ub'])\n",
    "\n",
    "    # ========================================================================\n",
    "    # Run the bin cutpoint pruning algorithm\n",
    "\n",
    "    # Flag the bins requiring amalgamation\n",
    "    df['_flag'] = 0\n",
    "    df = df.sort_values([id_var, '_bin'])\n",
    "        \n",
    "    # Allocate upwards for bins that are less than 20 count\n",
    "    df['_bad_up'] = 0\n",
    "    for b in range(2, 8):\n",
    "        df = df.sort_values([id_var, '_bin'])\n",
    "        df['_flag'] = np.where((df[id_var] == df[id_var].shift(1)) & (df['_bin'] == b) & (df['_count_'].shift(1) < mincount), 1, 0)\n",
    "        df['_count_'] = np.where((df[id_var] == df[id_var].shift(1)) & (df['_bin'] == b) & (df['_flag'] == 1), df['_count_'] + df['_count_'].shift(1), df['_count_'])\n",
    "        df['_bad_up'] = np.where((df[id_var] == df[id_var].shift(-1)) & (df['_bin'] == (b-1)) & (df['_flag'].shift(-1) == 1), 1, df['_bad_up'])\n",
    "        df['_count_'] = df['_count_'].where(df['_bad_up'] != 1, 0)\n",
    "                \n",
    "    # Allocate downwards for bins that are less than 20 count\n",
    "    df['_bad_dn'] = 0\n",
    "    for b in range(6, 0, -1):\n",
    "        df['_flag'] = np.where((df[id_var] == df[id_var].shift(-1)) & (df['_bin'] == b) & (df['_count_'].shift(-1) < mincount), 1, 0)\n",
    "        df['_count_'] = np.where((df[id_var] == df[id_var].shift(-1)) & (df['_bin'] == b) & (df['_flag'] == 1), df['_count_'] + df['_count_'].shift(-1), df['_count_'])\n",
    "        df['_bad_dn'] = np.where((df[id_var] == df[id_var].shift(1)) & (df['_bin'] == (b+1)) & (df['_flag'].shift(1) == 1), 1, df['_bad_dn'])\n",
    "        df['_count_'] = df['_count_'].where(df['_bad_dn'] != 1, 0)\n",
    "        \n",
    "    df.drop('_flag', axis=1, inplace=True)\n",
    "\n",
    "    # Define the amalgamated groups & collapse to get new endpoints\n",
    "    # Group the new bins together\n",
    "    df['_newbin'] = df.sort_values(by=[id_var, '_bin']).groupby([id_var]).cumcount() + 1\n",
    "    df['_newbin'] = df['_newbin'].where(~((df['_bad_up'] == 1) | (df['_bad_dn'] == 1)), None)\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    df.sort_values(by=[id_var, '_bin'], ascending=[True, False], inplace=True)\n",
    "\n",
    "    # Carry forward _newbin within each idvar\n",
    "    df['_newbin'] = df.groupby(id_var)['_newbin'].ffill()\n",
    "\n",
    "    # Sort the DataFrame again\n",
    "    df.sort_values(by=[id_var, '_bin'], inplace=True)\n",
    "\n",
    "    # Carry forward _newbin again within each idvar\n",
    "    df['_newbin'] = df.groupby(id_var)['_newbin'].ffill()\n",
    "\n",
    "    # Get the min/max edgepoints\n",
    "    result_df = df.groupby([id_var, '_newbin']).agg(\n",
    "        {'_lb':'min', '_ub':'max', '_count_':'sum'}).reset_index()\n",
    "\n",
    "    # Recode the bins so that they start at 1\n",
    "    result_df['_newbin'] = result_df.groupby(id_var).cumcount() + 1\n",
    "\n",
    "    # ========================================================================\n",
    "    # Clean the resulting dataset\n",
    "\n",
    "    # Format\n",
    "    result_df.rename(columns={'_newbin': '_bin'}, inplace=True)\n",
    "    result_df.sort_values(by=[id_var, '_bin'], inplace=True)\n",
    "    result_df['_maxbin'] = result_df.groupby(id_var)['_bin'].transform('max')\n",
    "\n",
    "    # Make sure there is a full set of bins for each ID\n",
    "    result_df = pd.merge(fullbins, result_df, how='outer', on=[id_var, '_bin'])\n",
    "\n",
    "    # Fill in the results from the merge above\n",
    "    result_df.sort_values(by=[id_var, '_bin'], inplace=True)\n",
    "    result_df['_maxbin'] = result_df.groupby(id_var)['_maxbin'].ffill()\n",
    "    \n",
    "    # Reshape the DataFrame wide by 'bin' and keep it long by 'idvar'\n",
    "    result_df = result_df.drop(columns='_count_')\n",
    "    wide_df = result_df.pivot(index=[id_var, '_maxbin'], columns='_bin').sort_index(axis=1, level=1)\n",
    "   \n",
    "    # Flatten the MultiIndex columns\n",
    "    wide_df.columns = [f'{col[0]}_{col[1]}' for col in wide_df.columns]\n",
    "\n",
    "    return wide_df.reset_index()\n",
    "\n",
    "\n",
    "def dynamic_spline(df, splineargs):\n",
    "    '''\n",
    "    Constructs a temperature spline with dynamic cutpoints allowing for\n",
    "    sufficient data to be included in each temperature bin. Relies on\n",
    "    temperature_bins() function to get the valid cutpoints\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas dataframe\n",
    "        Dataframe that has the required temperature var for spline\n",
    "        construction. Note that 'tempvar' must be a column in 'df'\n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : Pandas dataframe with the spline variables added\n",
    "    binlist : list of the column names in df that contain the spline vars\n",
    "\n",
    "    '''    \n",
    "        \n",
    "    # call the temperature bins function to get bin cutpoints\n",
    "    tempvar = splineargs['tempvar']\n",
    "    premvar = splineargs['idvars']\n",
    "    treatvar = splineargs['treatvar']\n",
    "    tempbins = dsa_temperature_bin(df[df[treatvar]==0], splineargs)\n",
    "\n",
    "    # merge the cutpoints back to the original dataset\n",
    "    df = df.merge(tempbins, on=premvar, how='right')\n",
    " \n",
    "    # Now construct the spline values\n",
    "    df['bin_1'] = np.minimum(df['_ub_1'], df[tempvar])  \n",
    "    for b in range(2, 8):\n",
    "        df[f'bin_{b}'] = np.where(df[tempvar] >= df[f'_lb_{b}'], \n",
    "                                  np.minimum(df[f'_ub_{b}'] - df[f'_lb_{b}'],\n",
    "                                  df[tempvar]-df[f'_lb_{b}']), 0.0)\n",
    "            \n",
    "    maxbin = int(df['_maxbin'].max())\n",
    "    binlist = ['bin_{}'.format(i) for i in range(1, maxbin)]\n",
    "        \n",
    "    return df, binlist \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Spline Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def towt(df, splineargs):\n",
    "    '''\n",
    "    Constructs and runs the spline regression for the given df\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        DF that contains the relevant usage and temperature (and optionally\n",
    "        GP) variables for the regression. Implicitly assumes that the df\n",
    "        being provided represents one customer/premise only\n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        the same input df, with additional column 'predicted' indicating\n",
    "        the output of the regression model fit\n",
    "\n",
    "    '''\n",
    "    # Parse the spline parameters\n",
    "    idvar = splineargs['idvars']\n",
    "    date = splineargs['date']\n",
    "    hour = splineargs['hour']\n",
    "    usage = splineargs['usagevar']\n",
    "    tempf = splineargs['tempvar']\n",
    "    treatment = splineargs['treatvar']\n",
    "    gps = splineargs['gps']\n",
    "    splinemethod = splineargs['splinemethod']\n",
    "    dailybool = splineargs['dailybool']\n",
    "    \n",
    "    # # Clean the data    \n",
    "    # df[date] = pd.to_datetime(df[date], format=\"%m/%d/%Y\")\n",
    "    # df = df.dropna()\n",
    "    \n",
    "    # Get the list of valid seasons\n",
    "    seasonlist = df['_season'].unique()\n",
    "\n",
    "    # instantiate the result dataframe    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    # loop through the seasons and run the regression on the subset\n",
    "    for season in seasonlist:\n",
    "        seasondf = df[df['_season'] == season]\n",
    "    \n",
    "        # Make the spline and merge things together\n",
    "        if splinemethod =='static':\n",
    "            seasondf, binlist = static_spline(seasondf, splineargs)\n",
    "               \n",
    "        if splinemethod == 'dynamic':\n",
    "            seasondf, binlist = dynamic_spline(seasondf, splineargs)\n",
    "            \n",
    "        # Construct the TOW/DOW factor variables\n",
    "        if not dailybool:\n",
    "            seasondf['how'] = seasondf[date].dt.dayofweek * 24 + seasondf[hour]\n",
    "            seasondf = pd.get_dummies(seasondf, columns=['how'])\n",
    "            \n",
    "            # Make the dependent variable list\n",
    "            dummies = ['how_{}'.format(h) for h in range(1, 169)]\n",
    "        \n",
    "        if dailybool:\n",
    "            seasondf['dow'] = seasondf[date].dt.dayofweek\n",
    "            seasondf = pd.get_dummies(seasondf, columns=['dow'])\n",
    "            \n",
    "            # Make the dependent variable list\n",
    "            dummies = ['dow_{}'.format(d) for d in range(0, 7)]       \n",
    "         \n",
    "        # Add the spline variables\n",
    "        indepvars = []\n",
    "        indepvars.extend(dummies)\n",
    "        indepvars.extend(binlist)\n",
    "        \n",
    "        # Add the GPs if applicable\n",
    "        if len(gps) > 0:\n",
    "            indepvars.extend(gps)\n",
    "        \n",
    "        # Construct the regression var list for the treatment period\n",
    "        x_all = seasondf[indepvars]\n",
    "\n",
    "        # Construct the regression var list for the training period\n",
    "        x = seasondf[indepvars][seasondf[treatment] == 0]\n",
    "        \n",
    "        # Construct the regression dependent variable    \n",
    "        y = seasondf[usage][seasondf[treatment] == 0]\n",
    "            \n",
    "        # Run the regression\n",
    "        model = sm.OLS(y.astype(float), x.astype(float)).fit()\n",
    "        predictions = model.predict(x_all)\n",
    "        \n",
    "        # Merge predictions back to main DF\n",
    "        seasondf = pd.merge(seasondf, predictions.to_frame(name='predicted'), left_index=True, right_index=True)\n",
    "        \n",
    "        # Clean up how dummies\n",
    "        seasondf = seasondf.drop(dummies, axis=1)\n",
    "        result = pd.concat([result, seasondf])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CVRMSE Computation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cvrmse(df, observed, predicted, splineargs):\n",
    "    '''\n",
    "    For a dataframe that contains ID, observed and predicted usage, compute\n",
    "    the CVRMSE for each ID\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        result DF from running the spline regression\n",
    "    observed : string\n",
    "        name of the observed usage variable in df\n",
    "    predicted : string\n",
    "        name of the predicted usage variable in df\n",
    "    splineargs : dict\n",
    "        regression parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        DF that contains one row per ID and the associated CVRMSE\n",
    "\n",
    "    '''\n",
    "    # Get the IDs to compute CVRMSE across\n",
    "    idvar = splineargs['idvars']\n",
    "    \n",
    "    # Keep only the ID, observed, and predicted values\n",
    "    df = df[[idvar, observed, predicted]]\n",
    "    \n",
    "    # Compute the error squared\n",
    "    df['sq_err'] = (df[predicted] - df[observed])**2\n",
    "    \n",
    "    # Groupby to get mean squared error\n",
    "    df = df.groupby([idvar]).mean()\n",
    "    \n",
    "    # Make RMSE and CVRMSE\n",
    "    df['rmse'] = np.sqrt((df['sq_err']))\n",
    "    df['cvrmse'] = df['rmse']/df[observed]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression parameters\n",
    "seasons = {'1':1, '2':1, '3':1, '4':2, '5':2, '6':3, \n",
    "           '7':3, '8':3, '9':3, '10':2, '11':2, '12':1}\n",
    "splineargs = {'idvars':'premise', 'date':'date', 'hour':'hour', \n",
    "              'treatvar':'treatment', 'gps':['gp_1', 'gp_2', 'gp_3', 'gp_4'], 'usagevar':'kwh', \n",
    "              'tempvar':'tempf', 'splinemethod':'dynamic', 'dailybool':True, \n",
    "              'seasons':seasons, 'mintempcount':20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(basepath + dir2 + analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hande data prep\n",
    "df = prep_data(df, splineargs)\n",
    "\n",
    "# Construct an empty placeholder for the results\n",
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loop through the full list of IDs and run the regression\n",
    "unique_values = df[splineargs['idvars']].unique()\n",
    "for acct in unique_values:\n",
    "    \n",
    "    # Subset the data to just that specific account\n",
    "    account_df = df[df[splineargs['idvars']]==acct]\n",
    "    \n",
    "    # Run the regression on that specific id\n",
    "    account_df = towt(account_df, splineargs)\n",
    "\n",
    "    # Append the results\n",
    "    results_df = pd.concat([results_df, account_df]) \n",
    "\n",
    "\n",
    "results_df.to_csv(basepath + dir2 + 'test_results.csv', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
